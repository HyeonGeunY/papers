{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeonGeunY/papers/blob/main/resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfauT5nQBvmL"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7tpkObHI2yb1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C4fjbwcRNHeW"
      },
      "outputs": [],
      "source": [
        "class ShortcutBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, stride: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, stride=stride, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.bn(self.conv(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GndjVgqZCV8i"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Identity()\n",
        "        if (stride != 1) or (in_channels != out_channels):\n",
        "            self.shortcut = ShortcutBlock(in_channels=in_channels, out_channels=out_channels, stride=stride)\n",
        "        \n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        shortcut = self.shortcut(x)\n",
        "        x = self.bn2(self.conv2(self.act1(self.bn1(self.conv1(x)))))\n",
        "        return self.act2(x + shortcut)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "49MR0gw5QmF_"
      },
      "outputs": [],
      "source": [
        "class BottleNeckBlock(nn.Module):\n",
        "    def __init__(self, in_channels, bottle_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, bottle_channels, kernel_size=1, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(bottle_channels)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(bottle_channels, bottle_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(bottle_channels)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(bottle_channels, out_channels, kernel_size=1, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "        self.act3 = nn.ReLU()\n",
        "\n",
        "        self.shortcut = nn.Identity()\n",
        "        if stride != 1 or (in_channels != out_channels):\n",
        "            self.shortcut = ShortcutBlock(in_channels, out_channels, stride=stride)\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        shortcut = self.shortcut(x)\n",
        "        x = self.act1(self.bn1(self.conv1(x)))\n",
        "        x = self.act2(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "        return self.act3(x + shortcut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BkZ8uxP0Tzvf"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_blocks: List[int], strides : List[int], channels: List[int], first_kernel_size: int = 7, image_channels: int = 3, n_classes=10, bottleneck: List[int] = None):\n",
        "        \"\"\"\n",
        "        MNIST 크기 (28 * 28)에 맞춰주기 위해 stride 크기 조정\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.firstconv = nn.Conv2d(image_channels, channels[0], kernel_size=first_kernel_size, stride=strides[0], padding=first_kernel_size//2)\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=3)\n",
        "        self.bottleneck = bottleneck\n",
        "        \n",
        "        self.layers = []\n",
        "        for i in range(1, len(channels)):\n",
        "            if self.bottleneck:\n",
        "                self.layers += self._repeat_bottleneck_layers(channels[i - 1], bottleneck[i - 1], channels[i], stride=strides[i], n_count=num_blocks[i - 1])\n",
        "            else:\n",
        "                self.layers += self._repeat_layers(channels[i - 1], channels[i], stride=strides[i], n_count=num_blocks[i - 1])\n",
        "\n",
        "        self.layers = nn.Sequential(*self.layers)\n",
        "        self.fc = nn.Linear(channels[i], n_classes)\n",
        "    \n",
        "\n",
        "    def _repeat_bottleneck_layers(self, in_channels, bottle_channels, out_channels, stride, n_count):\n",
        "        layers = []\n",
        "        strides = [stride] + [1] * (n_count - 1)\n",
        "\n",
        "        for i, s in enumerate(strides):\n",
        "            if i == 0:\n",
        "                layers.append(BottleNeckBlock(in_channels, bottle_channels, out_channels, s))\n",
        "            else:\n",
        "                layers.append(BottleNeckBlock(out_channels, bottle_channels, out_channels, s))\n",
        "                \n",
        "        return layers\n",
        "    \n",
        "\n",
        "    def _repeat_layers(self, in_channels, out_channels, stride, n_count):\n",
        "        layers = []\n",
        "        strides = [stride] + [1] * (n_count - 1)\n",
        "        for i, s in enumerate(strides):\n",
        "            if i == 0:\n",
        "                layers.append(ResidualBlock(in_channels, out_channels, s))\n",
        "            else:\n",
        "                layers.append(ResidualBlock(out_channels, out_channels, s))\n",
        "\n",
        "        return layers\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.bn1(self.firstconv(x)))\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.layers(x)\n",
        "        x = x.view(x.size(0), x.size(1), -1) # 2차원으로 만들어주기\n",
        "        x = x.mean(dim=-1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def resnet18_for_mnist():\n",
        "    return ResNet(num_blocks=[2, 2, 2, 2], strides=[1, 1, 2, 2, 2], channels=[64, 64, 128, 256, 512], image_channels=1)\n",
        "\n",
        "def resnet18_for_cifar():\n",
        "    return ResNet(num_blocks=[2, 2, 2, 2], strides=[1, 1, 2, 2, 2], channels=[64, 64, 128, 256, 512], image_channels=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyzAk-ydutJ5"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIgNBn7vvfGE"
      },
      "source": [
        "## 훈련 & 추론 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rvdztsHqvbx5"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "\n",
        "net = resnet18_for_mnist()\n",
        "net = net.to(device)\n",
        "net = torch.nn.DataParallel(net)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "learning_rate = 0.01\n",
        "file_name = 'resnet18_mnist.pt'\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0002) # weight_decay: L2 norm\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    \"\"\"\n",
        "    train code referenced from\n",
        "    https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/ResNet18_MNIST_Train.ipynb\n",
        "    \"\"\"\n",
        "    print('\\n[ Train epoch: %d ]' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device) # 모델과 데이터를 같은 머신에 탑재\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        benign_outputs = net(inputs) # benign_outputs (B, C)\n",
        "        loss = criterion(benign_outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = benign_outputs.max(1) # 값, 인덱스 반환\n",
        "\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item() # eq: 같은지 비교\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('\\nCurrent batch:', str(batch_idx))\n",
        "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
        "            print('Current benign train loss:', loss.item())\n",
        "\n",
        "    print('\\nTotal benign train accuarcy:', 100. * correct / total)\n",
        "    print('Total benign train loss:', train_loss)\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    print('\\n[ Test epoch: %d ]' % epoch)\n",
        "    net.eval() # 평가 모드로 전환\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device) # 같은 머신에 올리기\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print('\\nTest accuarcy:', 100. * correct / total)\n",
        "    print('Test average loss:', loss / total)\n",
        "\n",
        "    state = {\n",
        "        'net': net.state_dict()\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/' + file_name)\n",
        "    print('Model Saved!')\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lr = learning_rate\n",
        "    if epoch >= 5:\n",
        "        lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6f7_WIpuwFP",
        "outputId": "9b1311c5-8b92-4f17-a67d-bb4191686403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "17.9%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsHwczXLuxeE",
        "outputId": "2ac19cb8-d3d2-44b9-d2f1-7d8ff4a7332a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[ Train epoch: 0 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.078125\n",
            "Current benign train loss: 2.5015478134155273\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.29276028275489807\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.1344335824251175\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.07797230035066605\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.14628593623638153\n",
            "\n",
            "Total benign train accuarcy: 87.63333333333334\n",
            "Total benign train loss: 182.84281497448683\n",
            "\n",
            "[ Test epoch: 0 ]\n",
            "\n",
            "Test accuarcy: 96.63\n",
            "Test average loss: 0.0011120023612864315\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 1 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.020926665514707565\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.08177906274795532\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.04549944028258324\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.03265257179737091\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.0352836512029171\n",
            "\n",
            "Total benign train accuarcy: 97.4\n",
            "Total benign train loss: 41.302717104554176\n",
            "\n",
            "[ Test epoch: 1 ]\n",
            "\n",
            "Test accuarcy: 96.83\n",
            "Test average loss: 0.0009982649222714827\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 2 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.13073952496051788\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.08966673910617828\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.12195375561714172\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.04394968971610069\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.11417820304632187\n",
            "\n",
            "Total benign train accuarcy: 97.68333333333334\n",
            "Total benign train loss: 36.09897910989821\n",
            "\n",
            "[ Test epoch: 2 ]\n",
            "\n",
            "Test accuarcy: 97.84\n",
            "Test average loss: 0.0007001115036546253\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 3 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.1030123233795166\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.08215659111738205\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.05651557818055153\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.08632487803697586\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.04232766106724739\n",
            "\n",
            "Total benign train accuarcy: 97.84833333333333\n",
            "Total benign train loss: 33.05946615152061\n",
            "\n",
            "[ Test epoch: 3 ]\n",
            "\n",
            "Test accuarcy: 96.64\n",
            "Test average loss: 0.0011248492678627372\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 4 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.0758945643901825\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.11378495395183563\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.11140462756156921\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.044905465096235275\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.03952142968773842\n",
            "\n",
            "Total benign train accuarcy: 98.02166666666666\n",
            "Total benign train loss: 30.937236830592155\n",
            "\n",
            "[ Test epoch: 4 ]\n",
            "\n",
            "Test accuarcy: 96.7\n",
            "Test average loss: 0.0010827266332576982\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 5 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.05351578816771507\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.013556189835071564\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.007388303056359291\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.06269511580467224\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.013252198696136475\n",
            "\n",
            "Total benign train accuarcy: 99.14666666666666\n",
            "Total benign train loss: 13.556191536597908\n",
            "\n",
            "[ Test epoch: 5 ]\n",
            "\n",
            "Test accuarcy: 99.34\n",
            "Test average loss: 0.00019801925146894063\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 6 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0029604039154946804\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.004163385834544897\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.001373644219711423\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.016248980537056923\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0075047630816698074\n",
            "\n",
            "Total benign train accuarcy: 99.37833333333333\n",
            "Total benign train loss: 9.776716862223111\n",
            "\n",
            "[ Test epoch: 6 ]\n",
            "\n",
            "Test accuarcy: 99.3\n",
            "Test average loss: 0.000213328326192277\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 7 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.004914328455924988\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.005004760809242725\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.00715574761852622\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.01212308555841446\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.008664135821163654\n",
            "\n",
            "Total benign train accuarcy: 99.465\n",
            "Total benign train loss: 8.375783777504694\n",
            "\n",
            "[ Test epoch: 7 ]\n",
            "\n",
            "Test accuarcy: 99.4\n",
            "Test average loss: 0.0001832252582469664\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 8 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0036836627405136824\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.006350951734930277\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.01329946331679821\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.03121117874979973\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.004873592406511307\n",
            "\n",
            "Total benign train accuarcy: 99.53166666666667\n",
            "Total benign train loss: 7.31075936590787\n",
            "\n",
            "[ Test epoch: 8 ]\n",
            "\n",
            "Test accuarcy: 99.32\n",
            "Test average loss: 0.00019052222658428946\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 9 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.03711117431521416\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.002039131009951234\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.008596496656537056\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.02240978181362152\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.008158547803759575\n",
            "\n",
            "Total benign train accuarcy: 99.57833333333333\n",
            "Total benign train loss: 6.96579645620659\n",
            "\n",
            "[ Test epoch: 9 ]\n",
            "\n",
            "Test accuarcy: 99.09\n",
            "Test average loss: 0.0002818638403201476\n",
            "Model Saved!\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(0, 10):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cifar10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[ Train epoch: 0 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.078125\n",
            "Current benign train loss: 2.486156940460205\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.125\n",
            "Current benign train loss: 2.373959541320801\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.1171875\n",
            "Current benign train loss: 2.3941004276275635\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.0859375\n",
            "Current benign train loss: 2.4186313152313232\n",
            "\n",
            "Total benign train accuarcy: 10.42\n",
            "Total benign train loss: 936.6151392459869\n",
            "\n",
            "[ Test epoch: 0 ]\n",
            "\n",
            "Test accuarcy: 10.67\n",
            "Test average loss: 0.023933346605300904\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 1 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.125\n",
            "Current benign train loss: 2.3301987648010254\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.078125\n",
            "Current benign train loss: 2.3881874084472656\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.0546875\n",
            "Current benign train loss: 2.429082155227661\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.0546875\n",
            "Current benign train loss: 2.386240005493164\n",
            "\n",
            "Total benign train accuarcy: 10.01\n",
            "Total benign train loss: 937.656393289566\n",
            "\n",
            "[ Test epoch: 1 ]\n",
            "\n",
            "Test accuarcy: 10.7\n",
            "Test average loss: 0.023926548504829406\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 2 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.1015625\n",
            "Current benign train loss: 2.3995468616485596\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.1015625\n",
            "Current benign train loss: 2.3829777240753174\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.046875\n",
            "Current benign train loss: 2.4450876712799072\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.109375\n",
            "Current benign train loss: 2.4147720336914062\n",
            "\n",
            "Total benign train accuarcy: 10.238\n",
            "Total benign train loss: 937.136506319046\n",
            "\n",
            "[ Test epoch: 2 ]\n",
            "\n",
            "Test accuarcy: 10.58\n",
            "Test average loss: 0.023913226509094238\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 3 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.0546875\n",
            "Current benign train loss: 2.4468934535980225\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.1171875\n",
            "Current benign train loss: 2.3789727687835693\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.1015625\n",
            "Current benign train loss: 2.381430149078369\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.1171875\n",
            "Current benign train loss: 2.4120469093322754\n",
            "\n",
            "Total benign train accuarcy: 10.314\n",
            "Total benign train loss: 936.3484048843384\n",
            "\n",
            "[ Test epoch: 3 ]\n",
            "\n",
            "Test accuarcy: 10.75\n",
            "Test average loss: 0.023923622608184814\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 4 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.0703125\n",
            "Current benign train loss: 2.3975489139556885\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.125\n",
            "Current benign train loss: 2.3242735862731934\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.078125\n",
            "Current benign train loss: 2.3840813636779785\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.1015625\n",
            "Current benign train loss: 2.411616563796997\n",
            "\n",
            "Total benign train accuarcy: 9.956\n",
            "Total benign train loss: 937.0840899944305\n",
            "\n",
            "[ Test epoch: 4 ]\n",
            "\n",
            "Test accuarcy: 10.52\n",
            "Test average loss: 0.02394315128326416\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 5 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.1171875\n",
            "Current benign train loss: 2.32879376411438\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.109375\n",
            "Current benign train loss: 2.4095804691314697\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.1015625\n",
            "Current benign train loss: 2.4154083728790283\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.1015625\n",
            "Current benign train loss: 2.368032217025757\n",
            "\n",
            "Total benign train accuarcy: 10.214\n",
            "Total benign train loss: 937.1224265098572\n",
            "\n",
            "[ Test epoch: 5 ]\n",
            "\n",
            "Test accuarcy: 10.6\n",
            "Test average loss: 0.023924087977409362\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 6 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.09375\n",
            "Current benign train loss: 2.399003744125366\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.1171875\n",
            "Current benign train loss: 2.3994312286376953\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.0703125\n",
            "Current benign train loss: 2.4275963306427\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.125\n",
            "Current benign train loss: 2.4062082767486572\n",
            "\n",
            "Total benign train accuarcy: 10.072\n",
            "Total benign train loss: 937.2227749824524\n",
            "\n",
            "[ Test epoch: 6 ]\n",
            "\n",
            "Test accuarcy: 10.55\n",
            "Test average loss: 0.023934106373786925\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 7 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.1171875\n",
            "Current benign train loss: 2.3800745010375977\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.0859375\n",
            "Current benign train loss: 2.3924381732940674\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.0703125\n",
            "Current benign train loss: 2.401427984237671\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.125\n",
            "Current benign train loss: 2.3216934204101562\n",
            "\n",
            "Total benign train accuarcy: 10.328\n",
            "Total benign train loss: 936.3441119194031\n",
            "\n",
            "[ Test epoch: 7 ]\n",
            "\n",
            "Test accuarcy: 10.64\n",
            "Test average loss: 0.023907631254196166\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 8 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.0625\n",
            "Current benign train loss: 2.3961923122406006\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.0625\n",
            "Current benign train loss: 2.434566020965576\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.0625\n",
            "Current benign train loss: 2.447998285293579\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.1171875\n",
            "Current benign train loss: 2.402789354324341\n",
            "\n",
            "Total benign train accuarcy: 10.012\n",
            "Total benign train loss: 937.904673576355\n",
            "\n",
            "[ Test epoch: 8 ]\n",
            "\n",
            "Test accuarcy: 10.53\n",
            "Test average loss: 0.02391341886520386\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 9 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.0625\n",
            "Current benign train loss: 2.4643261432647705\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.109375\n",
            "Current benign train loss: 2.4209771156311035\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.15625\n",
            "Current benign train loss: 2.3321659564971924\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.140625\n",
            "Current benign train loss: 2.4380712509155273\n",
            "\n",
            "Total benign train accuarcy: 10.24\n",
            "Total benign train loss: 937.3711063861847\n",
            "\n",
            "[ Test epoch: 9 ]\n",
            "\n",
            "Test accuarcy: 10.54\n",
            "Test average loss: 0.023910407042503357\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 10 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.1328125\n",
            "Current benign train loss: 2.4160337448120117\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.09375\n",
            "Current benign train loss: 2.4697890281677246\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.109375\n",
            "Current benign train loss: 2.462991714477539\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.09375\n",
            "Current benign train loss: 2.4470372200012207\n",
            "\n",
            "Total benign train accuarcy: 10.22\n",
            "Total benign train loss: 936.7961239814758\n",
            "\n",
            "[ Test epoch: 10 ]\n",
            "\n",
            "Test accuarcy: 10.6\n",
            "Test average loss: 0.023923585724830627\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 11 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.109375\n",
            "Current benign train loss: 2.4036269187927246\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.1328125\n",
            "Current benign train loss: 2.3972160816192627\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.125\n",
            "Current benign train loss: 2.3738105297088623\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.1015625\n",
            "Current benign train loss: 2.3609416484832764\n",
            "\n",
            "Total benign train accuarcy: 10.246\n",
            "Total benign train loss: 936.9663310050964\n",
            "\n",
            "[ Test epoch: 11 ]\n",
            "\n",
            "Test accuarcy: 10.7\n",
            "Test average loss: 0.023937706661224366\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 12 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.1328125\n",
            "Current benign train loss: 2.3794655799865723\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.0859375\n",
            "Current benign train loss: 2.4804322719573975\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.1171875\n",
            "Current benign train loss: 2.4009203910827637\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.1015625\n",
            "Current benign train loss: 2.467949628829956\n",
            "\n",
            "Total benign train accuarcy: 10.322\n",
            "Total benign train loss: 937.5241940021515\n",
            "\n",
            "[ Test epoch: 12 ]\n",
            "\n",
            "Test accuarcy: 10.61\n",
            "Test average loss: 0.023936405968666075\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 13 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.0703125\n",
            "Current benign train loss: 2.4067330360412598\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.109375\n",
            "Current benign train loss: 2.3801562786102295\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.140625\n",
            "Current benign train loss: 2.4150640964508057\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.0859375\n",
            "Current benign train loss: 2.369338274002075\n",
            "\n",
            "Total benign train accuarcy: 10.232\n",
            "Total benign train loss: 937.6178855895996\n",
            "\n",
            "[ Test epoch: 13 ]\n",
            "\n",
            "Test accuarcy: 10.59\n",
            "Test average loss: 0.023928433895111085\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 14 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.09375\n",
            "Current benign train loss: 2.416264533996582\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.078125\n",
            "Current benign train loss: 2.4291326999664307\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.1171875\n",
            "Current benign train loss: 2.3564984798431396\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.125\n",
            "Current benign train loss: 2.3802502155303955\n",
            "\n",
            "Total benign train accuarcy: 10.224\n",
            "Total benign train loss: 936.9335536956787\n",
            "\n",
            "[ Test epoch: 14 ]\n",
            "\n",
            "Test accuarcy: 10.49\n",
            "Test average loss: 0.02393131833076477\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 15 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.1015625\n",
            "Current benign train loss: 2.41778564453125\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.09375\n",
            "Current benign train loss: 2.4458682537078857\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.1015625\n",
            "Current benign train loss: 2.392328977584839\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.1015625\n",
            "Current benign train loss: 2.423234701156616\n",
            "\n",
            "Total benign train accuarcy: 10.084\n",
            "Total benign train loss: 936.9967694282532\n",
            "\n",
            "[ Test epoch: 15 ]\n",
            "\n",
            "Test accuarcy: 10.57\n",
            "Test average loss: 0.023909785175323488\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 16 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.109375\n",
            "Current benign train loss: 2.4408881664276123\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.09375\n",
            "Current benign train loss: 2.453007698059082\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.0859375\n",
            "Current benign train loss: 2.443934679031372\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.078125\n",
            "Current benign train loss: 2.4588799476623535\n",
            "\n",
            "Total benign train accuarcy: 10.286\n",
            "Total benign train loss: 936.7682509422302\n",
            "\n",
            "[ Test epoch: 16 ]\n",
            "\n",
            "Test accuarcy: 10.51\n",
            "Test average loss: 0.023923263096809387\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 17 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.078125\n",
            "Current benign train loss: 2.4153215885162354\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.0625\n",
            "Current benign train loss: 2.375140905380249\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.1328125\n",
            "Current benign train loss: 2.433138370513916\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.09375\n",
            "Current benign train loss: 2.3674657344818115\n",
            "\n",
            "Total benign train accuarcy: 9.9\n",
            "Total benign train loss: 936.9946441650391\n",
            "\n",
            "[ Test epoch: 17 ]\n",
            "\n",
            "Test accuarcy: 10.67\n",
            "Test average loss: 0.023933473753929138\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 18 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.0859375\n",
            "Current benign train loss: 2.43510103225708\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.046875\n",
            "Current benign train loss: 2.4444656372070312\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.109375\n",
            "Current benign train loss: 2.329501152038574\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.09375\n",
            "Current benign train loss: 2.3521640300750732\n",
            "\n",
            "Total benign train accuarcy: 10.068\n",
            "Total benign train loss: 936.7912633419037\n",
            "\n",
            "[ Test epoch: 18 ]\n",
            "\n",
            "Test accuarcy: 10.53\n",
            "Test average loss: 0.023933641004562377\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 19 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.109375\n",
            "Current benign train loss: 2.3739538192749023\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.078125\n",
            "Current benign train loss: 2.372713565826416\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.0859375\n",
            "Current benign train loss: 2.35868501663208\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.109375\n",
            "Current benign train loss: 2.358410120010376\n",
            "\n",
            "Total benign train accuarcy: 10.258\n",
            "Total benign train loss: 936.5748372077942\n",
            "\n",
            "[ Test epoch: 19 ]\n",
            "\n",
            "Test accuarcy: 10.62\n",
            "Test average loss: 0.023910508227348328\n",
            "Model Saved!\n"
          ]
        }
      ],
      "source": [
        "file_name = 'resnet18_cifar10.pt'\n",
        "net = resnet18_for_cifar()\n",
        "net.to(device)\n",
        "for epoch in range(0, 20):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imagenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "resize = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(\n",
        "                    resize, scale=(0.5, 1.0)),      # resize 기준 0.5에서 1.0사이즈로 랜덤하게 크기 조정\n",
        "                transforms.RandomHorizontalFlip(), # 50% 확률로 반전 하거나 안하거나\n",
        "                transforms.ToTensor(),             # 텐서로 변환\n",
        "                transforms.Normalize(mean, std)    # 표준화\n",
        "                ])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "                transforms.ToTensor()])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMYnH1j2yyjUEH+omb4pySM",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "resnet.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "papers",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
