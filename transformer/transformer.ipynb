{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionUnit(nn.Module):\n",
    "    \"\"\"\n",
    "    multi-head-attention에서 key, query, value 위한 unit \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, heads: int, d_k: int, bias: bool):\n",
    "        \"\"\"\n",
    "        d_model: input 임배딩 차원\n",
    "        heads: head 개수\n",
    "        d_k: 각 head의 차원\n",
    "        논문에서는 d_model == heads * d_k\n",
    "        \"\"\"    \n",
    "        super().__init__()\n",
    "            \n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "        \n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: input vector (seq_len, batch_size, d_model) or (batch_size, d_model)\n",
    "        \n",
    "        output_size: (seq_len, batch_size, heads, d_k) or (batch_size, heads, d_k)\n",
    "        \"\"\"\n",
    "        head_shape = x.shape[:-1] # 연산 후 각 헤드 별로 분할하기 위함.\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = x.view(*head_shape, self.heads, self.d_k)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1, bias: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.query = MultiHeadAttentionUnit(d_model, heads, self.d_k, bias=bias)\n",
    "        self.value = MultiHeadAttentionUnit(d_model, heads, self.d_k, bias=bias)\n",
    "        self.key = MultiHeadAttentionUnit(d_model, heads, self.d_k, bias=bias)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)  \n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.scale = 1 / np.sqrt(self.d_k)\n",
    "        \n",
    "        self.attn = None # logging을 위한 attention 저장\n",
    "    \n",
    "    \n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        \"\"\"\n",
    "        i: ith seq\n",
    "        j: jth seq\n",
    "        b: batch_size\n",
    "        h: n_heads\n",
    "        \"\"\"\n",
    "        \n",
    "        return torch.einsum('ibhd, jbhd -> ijbh', query, key) \n",
    "    \n",
    "    def prepare_mask(self, mask: torch.Tensor, query_shape: List[int], key_shape: List[int]):\n",
    "        \"\"\"\n",
    "        mask: (seq_len_q, seq_len_k, batch_size)\n",
    "        query, key: (seq_len, batch_size, n_heads, d_model)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n",
    "        assert mask.shape[1] == key_shape[0]\n",
    "        assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
    "        \n",
    "        mask = mask.unsqueeze(-1) # 모든 헤드에 대해 동일한 마스크 확장\n",
    "        \n",
    "        return mask \n",
    "        \n",
    "    \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: \n",
    "                Optional[torch.Tensor]=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _ = query.shape # (seq_len_q, batch_size, d_model)\n",
    "        \n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = self.prepare_mask(mask, query.shape, key.shape)\n",
    "            \n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "        \n",
    "        scores = self.get_scores(query, key)\n",
    "        scores *= self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "            \n",
    "        attn = self.softmax(scores)\n",
    "        self.attn = attn.detach().clone()\n",
    "        attn = self.dropout()\n",
    "        \n",
    "        x = torch.einsum('ijbh,jbhd->ibhd', attn, value)\n",
    "        \n",
    "        x = x.reshape(seq_len, batch_size, -1) # n_heads * n_dim => d_model\n",
    "        \n",
    "        return self.ouput(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-papers",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
