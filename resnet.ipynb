{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYnH1j2yyjUEH+omb4pySM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeonGeunY/papers/blob/main/resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "DfauT5nQBvmL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7tpkObHI2yb1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ShortcutBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, stride: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, stride=stride, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.bn(self.conv(x))"
      ],
      "metadata": {
        "id": "C4fjbwcRNHeW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Identity()\n",
        "        if (stride != 1) or (in_channels != out_channels):\n",
        "            self.shortcut = ShortcutBlock(in_channels=in_channels, out_channels=out_channels, stride=stride)\n",
        "        \n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        shortcut = self.shortcut(x)\n",
        "        x = self.bn2(self.conv2(self.act1(self.bn1(self.conv1(x)))))\n",
        "        return self.act2(x + shortcut)\n"
      ],
      "metadata": {
        "id": "GndjVgqZCV8i"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BottleNeckBlock(nn.Module):\n",
        "    def __init__(self, in_channels, bottle_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, bottle_channels, kernel_size=1, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(bottle_channels)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(bottle_channels, bottle_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(bottle_channels)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(bottle_channels, out_channels, kernel_size=1, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "        self.act3 = nn.ReLU()\n",
        "\n",
        "        self.shortcut = nn.Identity()\n",
        "        if stride != 1 or (in_channels != out_channels):\n",
        "            self.shortcut = ShortcutBlock(in_channels, out_channels, stride=stride)\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        shortcut = self.shortcut(x)\n",
        "        x = self.act1(self.bn1(self.conv1(x)))\n",
        "        x = self.act2(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "        return self.act3(x + shortcut)"
      ],
      "metadata": {
        "id": "49MR0gw5QmF_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_blocks: List[int], strides : List[int], channels: List[int], first_kernel_size: int = 7, image_channels: int = 3, n_classes=10, bottleneck: List[int] = None):\n",
        "        \"\"\"\n",
        "        MNIST 크기 (28 * 28)에 맞춰주기 위해 stride 크기 조정\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.firstconv = nn.Conv2d(image_channels, channels[0], kernel_size=first_kernel_size, stride=strides[0], padding=first_kernel_size//2)\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=3)\n",
        "        self.bottleneck = bottleneck\n",
        "        \n",
        "        self.layers = []\n",
        "        for i in range(1, len(channels)):\n",
        "            if self.bottleneck:\n",
        "                self.layers += self._repeat_bottleneck_layers(channels[i - 1], bottleneck[i - 1], channels[i], stride=strides[i], n_count=num_blocks[i - 1])\n",
        "            else:\n",
        "                self.layers += self._repeat_layers(channels[i - 1], channels[i], stride=strides[i], n_count=num_blocks[i - 1])\n",
        "\n",
        "        self.layers = nn.Sequential(*self.layers)\n",
        "        self.fc = nn.Linear(channels[i], n_classes)\n",
        "    \n",
        "\n",
        "    def _repeat_bottleneck_layers(self, in_channels, bottle_channels, out_channels, stride, n_count):\n",
        "        layers = []\n",
        "        strides = [stride] + [1] * (n_count - 1)\n",
        "\n",
        "        for i, s in enumerate(strides):\n",
        "            if i == 0:\n",
        "                layers.append(BottleNeckBlock(in_channels, bottle_channels, out_channels, s))\n",
        "            else:\n",
        "                layers.append(BottleNeckBlock(out_channels, bottle_channels, out_channels, s))\n",
        "                \n",
        "        return layers\n",
        "    \n",
        "\n",
        "    def _repeat_layers(self, in_channels, out_channels, stride, n_count):\n",
        "        layers = []\n",
        "        strides = [stride] + [1] * (n_count - 1)\n",
        "        for i, s in enumerate(strides):\n",
        "            if i == 0:\n",
        "                layers.append(ResidualBlock(in_channels, out_channels, s))\n",
        "            else:\n",
        "                layers.append(ResidualBlock(out_channels, out_channels, s))\n",
        "\n",
        "        return layers\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.bn1(self.firstconv(x)))\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.layers(x)\n",
        "        x = x.view(x.size(0), x.size(1), -1) # 2차원으로 만들어주기\n",
        "        x = x.mean(dim=-1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def resnet18_for_mnist():\n",
        "    return ResNet(num_blocks=[2, 2, 2, 2], strides=[1, 1, 2, 2, 2], channels=[64, 64, 128, 256, 512], image_channels=1)"
      ],
      "metadata": {
        "id": "BkZ8uxP0Tzvf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습"
      ],
      "metadata": {
        "id": "pyzAk-ydutJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6f7_WIpuwFP",
        "outputId": "9b1311c5-8b92-4f17-a67d-bb4191686403"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 훈련 & 추론 함수"
      ],
      "metadata": {
        "id": "gIgNBn7vvfGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "\n",
        "net = resnet18_for_mnist()\n",
        "net = net.to(device)\n",
        "net = torch.nn.DataParallel(net)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "learning_rate = 0.01\n",
        "file_name = 'resnet18_mnist.pt'\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0002) # weight_decay: L2 norm\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    \"\"\"\n",
        "    train code referenced from\n",
        "    https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/ResNet18_MNIST_Train.ipynb\n",
        "    \"\"\"\n",
        "    print('\\n[ Train epoch: %d ]' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device) # 모델과 데이터를 같은 머신에 탑재\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        benign_outputs = net(inputs) # benign_outputs (B, C)\n",
        "        loss = criterion(benign_outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = benign_outputs.max(1) # 값, 인덱스 반환\n",
        "\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item() # eq: 같은지 비교\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('\\nCurrent batch:', str(batch_idx))\n",
        "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
        "            print('Current benign train loss:', loss.item())\n",
        "\n",
        "    print('\\nTotal benign train accuarcy:', 100. * correct / total)\n",
        "    print('Total benign train loss:', train_loss)\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    print('\\n[ Test epoch: %d ]' % epoch)\n",
        "    net.eval() # 평가 모드로 전환\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device) # 같은 머신에 올리기\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print('\\nTest accuarcy:', 100. * correct / total)\n",
        "    print('Test average loss:', loss / total)\n",
        "\n",
        "    state = {\n",
        "        'net': net.state_dict()\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/' + file_name)\n",
        "    print('Model Saved!')\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lr = learning_rate\n",
        "    if epoch >= 5:\n",
        "        lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "rvdztsHqvbx5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0, 10):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsHwczXLuxeE",
        "outputId": "2ac19cb8-d3d2-44b9-d2f1-7d8ff4a7332a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ Train epoch: 0 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.046875\n",
            "Current benign train loss: 2.4314069747924805\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.1369875967502594\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.11663743853569031\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.1703571081161499\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.06594353169202805\n",
            "\n",
            "Total benign train accuarcy: 90.59\n",
            "Total benign train loss: 148.6219130679965\n",
            "\n",
            "[ Test epoch: 0 ]\n",
            "\n",
            "Test accuarcy: 93.02\n",
            "Test average loss: 0.002262835589516908\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 1 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.18407894670963287\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.24108824133872986\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.09336555749177933\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.15133006870746613\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.024779390543699265\n",
            "\n",
            "Total benign train accuarcy: 97.24833333333333\n",
            "Total benign train loss: 43.498177940025926\n",
            "\n",
            "[ Test epoch: 1 ]\n",
            "\n",
            "Test accuarcy: 97.1\n",
            "Test average loss: 0.0009880258423276245\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 2 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.08795677870512009\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.09068668633699417\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.13497644662857056\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.1626758575439453\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.05007891729474068\n",
            "\n",
            "Total benign train accuarcy: 97.62833333333333\n",
            "Total benign train loss: 37.70055399276316\n",
            "\n",
            "[ Test epoch: 2 ]\n",
            "\n",
            "Test accuarcy: 98.69\n",
            "Test average loss: 0.00044590271906927225\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 3 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.05193920433521271\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03284493833780289\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.06044533848762512\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.03575433790683746\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.07928328961133957\n",
            "\n",
            "Total benign train accuarcy: 97.84\n",
            "Total benign train loss: 34.10792813077569\n",
            "\n",
            "[ Test epoch: 3 ]\n",
            "\n",
            "Test accuarcy: 97.87\n",
            "Test average loss: 0.000686283629015088\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 4 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02924933098256588\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.02279595471918583\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.07335401326417923\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.06511251628398895\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.03856154531240463\n",
            "\n",
            "Total benign train accuarcy: 98.015\n",
            "Total benign train loss: 29.969136386644095\n",
            "\n",
            "[ Test epoch: 4 ]\n",
            "\n",
            "Test accuarcy: 97.71\n",
            "Test average loss: 0.0007469677416142076\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 5 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.07789504528045654\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.0546656958758831\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.015614381991326809\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03739986568689346\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0037053083069622517\n",
            "\n",
            "Total benign train accuarcy: 99.12333333333333\n",
            "Total benign train loss: 13.878208511741832\n",
            "\n",
            "[ Test epoch: 5 ]\n",
            "\n",
            "Test accuarcy: 99.29\n",
            "Test average loss: 0.00021277395183278713\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 6 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.042257025837898254\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.04700928181409836\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.01408602949231863\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.005425178911536932\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.08147536218166351\n",
            "\n",
            "Total benign train accuarcy: 99.36833333333334\n",
            "Total benign train loss: 10.097152928938158\n",
            "\n",
            "[ Test epoch: 6 ]\n",
            "\n",
            "Test accuarcy: 99.43\n",
            "Test average loss: 0.00019539319913164946\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 7 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.027642713859677315\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.004110035952180624\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.025438755750656128\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0045522344298660755\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.007231803145259619\n",
            "\n",
            "Total benign train accuarcy: 99.46833333333333\n",
            "Total benign train loss: 8.34800997725688\n",
            "\n",
            "[ Test epoch: 7 ]\n",
            "\n",
            "Test accuarcy: 99.39\n",
            "Test average loss: 0.00021352598991506965\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 8 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0034138106275349855\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.007809988688677549\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.048276886343955994\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.014803373254835606\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0049902210012078285\n",
            "\n",
            "Total benign train accuarcy: 99.51833333333333\n",
            "Total benign train loss: 7.720793271262664\n",
            "\n",
            "[ Test epoch: 8 ]\n",
            "\n",
            "Test accuarcy: 99.31\n",
            "Test average loss: 0.0002275848618359305\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 9 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.011027644388377666\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0073541817255318165\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.04300752654671669\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.01209237053990364\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.017825543880462646\n",
            "\n",
            "Total benign train accuarcy: 99.63166666666666\n",
            "Total benign train loss: 6.121854324825108\n",
            "\n",
            "[ Test epoch: 9 ]\n",
            "\n",
            "Test accuarcy: 99.29\n",
            "Test average loss: 0.0002314966288562573\n",
            "Model Saved!\n"
          ]
        }
      ]
    }
  ]
}